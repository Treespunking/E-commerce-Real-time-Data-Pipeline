# docker-compose.yml
version: '3.7'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"

  kafka:
    image: confluentinc/cp-kafka:7.0.0
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: PLAINTEXT://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_DEFAULT_REPLICATION_FACTOR: 1
      KAFKA_MIN_INSYNC_REPLICAS: 1
      CONFLUENT_SUPPORT_METRICS_ENABLE: 'false'
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 100

  kafka-init:
    image: confluentinc/cp-kafka:7.0.0
    depends_on:
      - kafka
    command: >
      sh -c "
        echo 'Waiting for Kafka to be ready...' &&
        until python3 -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.settimeout(5); s.connect(('kafka', 9092)); s.close()\" 2>/dev/null; do
          echo 'Kafka not ready, retrying in 2 seconds...' &&
          sleep 2
        done &&
        echo 'Creating topic: ecommerce_events' &&
        kafka-topics --create --if-not-exists --topic ecommerce_events --bootstrap-server kafka:9092 --partitions 1 --replication-factor 1 &&
        echo 'Topic created or already exists.'
      "
    networks:
      - default

  spark:
    image: bitnami/spark:3.5.2
    depends_on:
      - kafka
    volumes:
      - ./spark-apps:/opt/spark-apps
    env_file:
      - .env
    environment:
      - SPARK_MODE=client
    command: |
      bash -c '
        # Install python-dotenv only if needed (optional, since .env is loaded via env_file)
        pip install python-dotenv --user > /dev/null 2>&1 || echo "python-dotenv is optional or already installed"

        echo "Waiting for Kafka..."
        until python3 -c "import socket; s = socket.socket(); s.connect((\"kafka\", 9092)); s.close()" 2>/dev/null; do
          echo "Retrying Kafka connection..."
          sleep 2
        done

        echo "Submitting Spark job to write Iceberg tables to S3..."
        spark-submit \
          --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.2,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.5.2,org.apache.hadoop:hadoop-aws:3.3.4 \
          --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \
          --conf spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog \
          --conf spark.sql.catalog.local.type=hadoop \
          --conf spark.sql.catalog.local.warehouse=s3a://${ICEBERG_S3_BUCKET}/iceberg-warehouse \
          --conf spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem \
          --conf spark.hadoop.fs.s3a.aws.credentials.provider=com.amazonaws.auth.DefaultAWSCredentialsProviderChain \
          --conf spark.hadoop.fs.s3a.fast.upload=true \
          --conf spark.sql.streaming.checkpointLocation=s3a://${ICEBERG_S3_BUCKET}/checkpoint/ecommerce-stream \
          /opt/spark-apps/spark_streaming.py
      '
    networks:
      - default

  api:
    build: ./api
    ports:
      - "8000:8000"
    depends_on:
      - kafka
    environment:
      - KAFKA_BOOTSTRAP_SERVER=kafka:9092
    command: >
      sh -c "
        echo 'Waiting for Kafka (kafka:9092) to be ready...' &&
        until python3 -c \"import socket; s = socket.socket(socket.AF_INET, socket.SOCK_STREAM); s.settimeout(5); s.connect(('kafka', 9092)); s.close()\" 2>/dev/null; do
          echo 'Kafka not ready, retrying in 2 seconds...' &&
          sleep 2
        done &&
        echo 'Kafka is ready! Starting FastAPI...' &&
        uvicorn main:app --host 0.0.0.0 --port 8000
      "
    networks:
      - default

networks:
  default:
    driver: bridge